In a discussion about threats posed by AI systems, Sam Altman, OpenAIâ€™s CEO and co-founder, has confirmed that the company is not currently training GPT-5, the presumed successor to its AI language model GPT-4, released this March. Speaking at an event at MIT, Altman was asked about a recent open letter circulated among the tech world that requested that labs like OpenAI pause development of AI systems â€œmore powerful than GPT-4.â€ The letter highlighted concerns about the safety of future systems but has been criticized by many in the industry, including a number of signatories. Experts disagree about the nature of the threat posed by AI (is it existential or more mundane?) as well as how the industry might go about â€œpausingâ€ development in the first place. At MIT, Altman said the letter was â€œmissing most technical nuance about where we need the pauseâ€ and noted that an earlier version claimed that OpenAI is currently training GPT-5. â€œWe are not and wonâ€™t for some time,â€ said Altman. â€œSo in that sense it was sort of silly.â€ However, just because OpenAI is not working on GPT-5 doesnâ€™t mean itâ€™s not expanding the capabilities of GPT-4 â€”Â or, as Altman was keen to stress, considering the safety implications of such work. â€œWe are doing other things on top of GPT-4 that I think have all sorts of safety issues that are important to address and were totally left out of the letter,â€ he said. You can watch a video of the exchange below: Altmanâ€™s comments are interesting â€” though not necessarily because of what they reveal about OpenAIâ€™s future plans. Instead, they highlight a significant challenge in the debate about AI safety: the difficulty of measuring and tracking progress. Altman may say that OpenAI is not currently training GPT-5, but thatâ€™s not a particularly meaningful statement. Some of the confusion can be attributed to what I call the fallacy of version numbers: the idea that numbered tech updates reflect definite and linear improvements in capability. Itâ€™s a misconception thatâ€™s been nurtured in the world of consumer tech for years, where numbers assigned to new phones or operating systems aspire to the rigor of version control but are really just marketing tools. â€œWell of course the iPhone 35 is better than the iPhone 34,â€ goes the logic of this system. â€œThe number is bigger ipso facto the phone is better.â€ Because of the overlap between the worlds of consumer tech and artificial intelligence, this same logic is now often applied to systems like OpenAIâ€™s language models. This is true not only of the sort of hucksters who post hyperbolic ğŸ¤¯ Twitter threads ğŸ¤¯Â predicting that superintelligent AI will be here in a matter of years because the numbers keep getting bigger but also of more informed and sophisticated commentators. As a lot of claims made about AI superintelligence are essentially unfalsifiable, these individuals rely on similar rhetoric to get their point across. They draw vague graphs with axes labeled â€œprogressâ€ and â€œtime,â€ plot a line going up and to the right, and present this uncritically as evidence. This is not to dismiss fears about AI safety or ignore the fact that these systems are rapidly improving and not fully under our control. But it is to say that there are good arguments and bad arguments, and just because weâ€™ve given a number to something â€” be that a new phone or the concept of intelligence â€” doesnâ€™t mean we have the full measure of it. Instead, I think the focus in these discussions should be on capabilities: on demonstrations of what these systems can and canâ€™t do and predictions of how this may change over time. Thatâ€™s why Altmanâ€™s confirmation that OpenAI is not currently developing GPT-5 wonâ€™t be of any consolation to people worried about AI safety. The company is still expanding the potential of GPT-4 (by connecting it to the internet, for example), and others in the industry are building similarly ambitious tools, letting AI systems act on behalf of users. Thereâ€™s also all sorts of work that is no doubt being done to optimize GPT-4, and OpenAI may release GPT-4.5 (as it did GPT-3.5) first â€” another way that version numbers can mislead. Even if the worldâ€™s governments were somehow able to enforce a ban on new AI developments, itâ€™s clear that society has its hands full with the systems currently available. Sure, GPT-5 isnâ€™t coming yet, but does it matter when GPT-4 is still not fully understood? 