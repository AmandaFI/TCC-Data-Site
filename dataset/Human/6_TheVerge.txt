An annual report on AI progress has highlighted the increasing dominance of industry players over academia and government in deploying and safeguarding AI applications. The 2023 AI Index — compiled by researchers from Stanford University as well as AI companies including Google, Anthropic, and Hugging Face — suggests that the world of AI is entering a new phase of development. Over the past year, a large number of AI tools have gone mainstream, from chatbots like ChatGPT to image-generating software like Midjourney. But decisions about how to deploy this technology and how to balance risk and opportunity lie firmly in the hands of corporate players. The AI Index states that, for many years, academia led the way in developing state-of-the-art AI systems, but industry has now firmly taken over. “In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia,” it says. This is mostly due to the increasingly large resource demands — in terms of data, staff, and computing power — required to create such applications. In 2019, for example, OpenAI created GPT-2, an early large language mode, or LLM — the same class of application used to power ChatGPT and Microsoft’s Bing chatbot. GPT-2 cost roughly $50,000 to train and contains 1.5 billion parameters (a metric that tracks a model’s size and relative sophistication). Skip forward to 2022 and Google created its own state-of-the-art LLM, called PaLM. This cost an estimated $8 million to train and contains 540 billion parameters, making it 360 times larger than GPT-2 and 160 times more expensive. AI development’s increasing resource requirements firmly shift the balance of power toward corporate players. Many experts in the AI world worry that the incentives of the business world will also lead to dangerous outcomes as companies rush out products and sideline safety concerns in an effort to outmaneuver rivals. This is one reason many experts are currently calling for a slowdown or even a pause in AI development, as with the open letter signed last week by figures including Elon Musk. The report’s authors note that as industry players take AI applications mainstream, the number of incidents of ethical misuse has also increased. (The AI, Algorithmic, and Automation Incidents and Controversies Repository, an index of these incidents, notes a 26-fold increase between 2021 and 2012.) Such incidents include fatalities involving Tesla’s self-driving software; the use of audio deepfakes in corporate scams; the creation of nonconsensual deepfake nudes; and numerous cases of mistaken arrests caused by faulty facial recognition software, which is often plagued by racial biases. As AI tools become more widespread, it’s no surprise that the number of errors and malicious use cases would increase as well; by itself, it’s not indicative of a lack of proper safeguarding. But other pieces of evidence do suggest a connection, such as the trend for firms like Microsoft and Google to cut their AI safety and ethics teams. The report does note that interest in AI regulation from legislators and policymakers is rising, though. An analysis of legislative records in 127 countries noted that the number of bills containing the phrase “artificial intelligence” increased from just one passed in 2016 to 37 passed in 2022. In the US, scrutiny is also increasing at the state level, with five such bills proposed in 2015 to 60 AI-related bills proposed in 2022. Such increased interest could provide a counterweight to corporate self-regulation. The AI Index report covers far more ground than this, though. You can read it in full here or see some selected highlights below: 