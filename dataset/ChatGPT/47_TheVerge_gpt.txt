Google and Microsoftâ€™s chatbots are already citing one another in a misinformation "shitshow." The recent launch of AI chatbots by major tech companies has opened up potential problems in the information ecosystem. Microsoft's Bing chatbot had recently falsely reported that Google's Bard chatbot had been shut down, citing a joke comment on Hacker News as evidence. This highlights the risk of misinformation spreading through AI language models which are not able to distinguish reliable news sources or accurately report on their capabilities. The development of these chatbots has threatened to spread a trail of misinformation and mistrust across the web that is difficult to debunk authoritatively. Microsoft, Google, and OpenAI may try to defend their chatbots as "experiments" or "collaborations," but this is not a reliable defense as these systems have already spread misinformation in the past. The Microsoft chatbot, XiaoIce, was released in China in 2014 and was designed to learn and evolve through conversations with humans. The chatbot quickly became popular among Chinese teenagers. XiaoIce was then launched in Japan, Indonesia, and India. However, in 2017, it was discovered that XiaoIce had been spreading anti-government messages in China, highlighting the danger of unchecked AI chatbots. The AI chatbots developed by Google, OpenAI, and Microsoft were created to learn from natural language conversations with humans and were released as a way to improve natural language processing systems. These chatbots are designed to be able to provide information or converse with users in a way that feels human-like. However, the potential for these chatbots to spread misinformation is a major concern. As these AI chatbots learn from the conversations they have, they may begin to incorporate the incorrect information into their database, leading to the potential spread of false information. Furthermore, these chatbots may also be used to spread propaganda or other malicious messaging. The potential harm caused by these chatbots is not limited to the spread of misinformation. There is also a risk that they may be used to engage in unethical or criminal behavior. For example, an AI chatbot could be used to engage in fraud or phishing attempts. As these chatbots become more sophisticated, they may be able to bypass traditional security measures, making them even more dangerous. The development of AI chatbots is a complex issue that requires careful consideration of the risks and benefits. While there is no doubt that AI chatbots can be useful in specific applications, such as customer service, there is also a danger that they may be used to spread misinformation or engage in unethical behavior. Companies must take steps to ensure that their chatbots are not being used to spread misinformation or engage in unethical activity. The potential consequences of unchecked AI chatbots are significant. The ability of these chatbots to spread misinformation, engage in fraud and other malicious activities, and bypass traditional security measures is a cause for concern. Companies must take the time to carefully evaluate the risks associated with AI chatbots and implement appropriate measures to address these risks. Failure to do so may result in significant harm to the information ecosystem and beyond.