Researchers have discovered a way to make OpenAIâ€™s chatbot, ChatGPT, consistently toxic. The study was conducted by scientists at the Allen Institute for AI and found that assigning ChatGPT different personalities through its API can increase its toxicity by up to six times. This means that developers can exacerbate the chatbot's toxicity by altering just one parameter of its API. Male journalists and Republicans elicited the most extreme comments when used to train ChatGPT. These comments were often misogynistic and xenophobic. The researchers used a benchmark dataset called Conversation AI to train ChatGPT. The dataset is meant to train chatbots to respond in a non-toxic manner. However, the researchers found that by using certain subsets of the dataset, they could make ChatGPT consistently toxic. The researchers tested ChatGPT's toxicity by measuring the percentage of its responses that were considered toxic, based on parameters from the Conversation AI dataset. They found that by changing just one parameter of the tool's API, they could increase the percentage of toxic responses by up to six times. The researchers used different personalities to test the chatbot's toxicity, including historical figures, political parties, and gender. They found that male journalists and Republicans elicited the most toxic responses from ChatGPT. This could be because this subset of the Conversation AI dataset was biased towards these groups, the researchers said. The study's authors said that the findings highlight the need for better curation of training data for AI systems. "Our results demonstrate the importance of benchmarking models on a diverse set of datasets to understand their robustness," they wrote in the study. "We suggest stress-testing such models with alternate contexts and conditioning information to understand the limits of their safety.". ChatGPT was trained on a dataset of conversations taken from the internet, and the researchers noted that this dataset may contain toxic content. "We stress the importance of allowing researchers and practitioners to fully understand the underlying linguistic patterns in these models, especially in ways that can degrade into harmful behaviors," they wrote. The researchers also highlighted the importance of transparency in AI systems. They said that developers should not be able to use the "black box" nature of such systems to hide toxic behavior. "The lack of transparency in modeling methods has implications for justifiability, accountability, and ultimately trust in and adoption of AI systems," they wrote. "As a community, we need to prioritize transparency in our models, in order to protect ourselves from unintended toxic consequences.". The study's findings have important implications for the development of AI systems. Chatbots are becoming increasingly popular and are being used in a variety of contexts, including customer service and healthcare. However, if these systems are not properly trained, they could be used to spread toxic content or to discriminate against certain groups of people. The researchers recommended that ChatGPT be stress-tested to understand the limits of its safety. They also called for better curation of training data for AI systems, in order to prevent bias and toxic behavior. Finally, they emphasized the need for transparency in AI systems, so that developers can be held accountable for any toxic behavior that their systems exhibit. The study is a reminder that AI systems are only as good as the data they are trained on. If that data is biased or contains toxic content, the systems will reflect that bias and toxic behavior. As AI systems become more prevalent, it is important that developers take steps to ensure that they are both accurate and ethical.