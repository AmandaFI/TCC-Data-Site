Facebook content moderators are responsible for filtering through undesirable content, such as graphic violence, hate speech, and child exploitation. However, the psychological cost of constant exposure to this type of content is taking its toll on these employees. Trevin Brownie, a content moderator based in Kenya, is taking legal action against Meta, the parent company of Facebook, claiming that his job has "cost him his humanity" and that he needs more support for his mental health. Brownie is one of 184 content moderators supported by campaign group Foxglove, who claim that they were unfairly treated and discriminated against after complaining about working conditions and attempting to form a union. In January, Brownie, along with 260 others, was laid off from Facebook's content moderation team. The psychological toll of content moderation is not unique to Brownie or his colleagues in Kenya. Facebook content moderators in other countries have also made similar claims of emotional distress and trauma. In fact, content moderators at Facebook have described the job as "The most toxic, dangerous thing on the internet". The issue has brought to light a larger problem facing tech companies, who outsource content moderation to countries like Kenya, where labor is cheaper. Content moderators are often exposed to a large quantity of disturbing and traumatising material while being provided with little to no support for their mental health. Brownie claims that he was not provided with any counselling or therapy during his time as a content moderator. He said that he did not fully grasp the psychological toll of the job until after he had left. In an interview with The Guardian, Brownie said: "I realised that the job had deadened my feelings, I was not able to feel anything. It had cost me my humanity.". Following his departure from Facebook, Brownie suffered from anxiety, depression, and PTSD. He said that he was not able to sleep and had lost his appetite. Brownie also reported experiencing suicidal thoughts. Facebook has tried to distance itself from the dispute, stating that the company is "committed to providing a safe and supportive workplace". However, a recent ruling means that Meta could be sued for unfair termination. Foxglove, the campaign group supporting Brownie and his colleagues, claims that Facebook's content moderation process is inherently flawed. The group has called on Facebook to take responsibility for the well-being of its content moderators, and to provide them with the necessary resources to deal with the psychological toll of their job. In response to the claims made by Brownie and his colleagues, Facebook has stated that it is committed to improving the well-being of its content moderators. The company has implemented a variety of measures to support content moderators, including the use of AI to detect harmful content and the implementation of a "well-being guide" to provide support for moderators. However, critics argue that these measures are not enough, and that Facebook needs to take more substantive action to improve the well-being of its content moderators. The issue has sparked a broader debate around the responsibility of tech companies to provide for the mental health of their employees, particularly in the context of content moderation. As the debate continues, content moderators like Brownie are left struggling with the psychological toll of their job. While it remains to be seen whether or not Brownie's legal action will be successful, it has brought much-needed attention to the issue and has forced Facebook to confront the realities of the psychological toll of content moderation.